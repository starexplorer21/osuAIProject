{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-16T07:39:19.571611100Z",
     "start_time": "2025-01-16T07:39:14.243545100Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from torch.distributions.categorical import Categorical\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import sklearn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def make_env(env_id, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-16T07:39:19.587629700Z",
     "start_time": "2025-01-16T07:39:19.572616200Z"
    }
   },
   "id": "cdecbf8bd4194341"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# initializing layers with better starting weights for training. \n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-16T07:39:19.606333700Z",
     "start_time": "2025-01-16T07:39:19.589629900Z"
    }
   },
   "id": "12f7c4928a1865dd"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        \n",
    "        self.mult = torch.tensor([512,384])\n",
    "        self.mult = self.mult.to(device)\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(in_channels=4, out_channels=64, kernel_size=4)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            layer_init(nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(1),\n",
    "            layer_init(nn.Linear(378, 64)),\n",
    "            nn.Sigmoid(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Sigmoid(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(in_channels=4, out_channels=64, kernel_size=4)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            layer_init(nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4)),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(1),\n",
    "            layer_init(nn.Linear(378, 64)),\n",
    "            nn.Sigmoid(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Sigmoid(),\n",
    "            layer_init(nn.Linear(64, 2), std=0.01),\n",
    "        )\n",
    "        self.logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        means = self.actor(x)\n",
    "        logstd = self.logstd.expand_as(means)\n",
    "        std = torch.exp(logstd)\n",
    "        probs = torch.distributions.Normal(means, std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-16T07:54:04.694019400Z",
     "start_time": "2025-01-16T07:54:04.687018900Z"
    }
   },
   "id": "f0c0b01f82200d1b"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "class OsuEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.misses = 0\n",
    "        self.count = 0\n",
    "        self.single_action_space = gym.spaces.Box(0, 512, [2])\n",
    "        self.single_observation_space = gym.spaces.Box(0, 1, [4, 68, 120])\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        return self.data['data'].iloc[self.count]\n",
    "    \n",
    "    def _get_info(self):\n",
    "        return self.misses, self.count\n",
    "    \n",
    "    def reset(self):\n",
    "        # reset env, so set misses to 0 and counter back to 0\n",
    "        \n",
    "        self.misses = 0\n",
    "        self.count = 0\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # reward is just a recalced loss. \n",
    "        # the maximium possible score is 438400, so subtract from 1/2 so close values are positive and far are negative. \n",
    "        gt = np.array(self.data['labels'].iloc[self.count])\n",
    "        reward = 219200 - np.linalg.norm(action-gt)\n",
    "        terminated = False\n",
    "        # An environment is completed if and only if the agent has finished the map or dies\n",
    "        if self.misses == 5:\n",
    "            terminated = True\n",
    "            # dying is bad\n",
    "            reward += -10000\n",
    "        if self.count == self.data.shape[0]:\n",
    "            terminated = True\n",
    "            # finishing is good. \n",
    "            reward += 10000\n",
    "        # i never truncate\n",
    "        truncated = False\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        self.count += 1\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-16T08:10:38.714200400Z",
     "start_time": "2025-01-16T08:10:38.705172800Z"
    }
   },
   "id": "29e907c7e277e56e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "interpret arguments and setup wandb writer (online dashboard for monitoring). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "376a2204cc13b388"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# This was a dataclass I found from clean RL and organizes all the values in a convenient way. \n",
    "# the code required significant debugging to function in jupyter notebook. \n",
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = 'OsuPPO'\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 39\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = True\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    track: bool = False\n",
    "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
    "    wandb_project_name: str = \"cleanRL\"\n",
    "    \"\"\"the wandb's project name\"\"\"\n",
    "    wandb_entity: str = None\n",
    "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
    "    capture_video: bool = True\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"LunarLander-v3\"\n",
    "    \"\"\"the id of the environment\"\"\"\n",
    "    total_timesteps: int = 4000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 2.5e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    num_envs: int = 4\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    num_steps: int = 128\n",
    "    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n",
    "    anneal_lr: bool = True\n",
    "    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    gae_lambda: float = 0.95\n",
    "    \"\"\"the lambda for the general advantage estimation\"\"\"\n",
    "    num_minibatches: int = 4\n",
    "    \"\"\"the number of mini-batches\"\"\"\n",
    "    update_epochs: int = 4\n",
    "    \"\"\"the K epochs to update the policy\"\"\"\n",
    "    norm_adv: bool = True\n",
    "    \"\"\"Toggles advantages normalization\"\"\"\n",
    "    clip_coef: float = 0.2\n",
    "    \"\"\"the surrogate clipping coefficient\"\"\"\n",
    "    clip_vloss: bool = True\n",
    "    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n",
    "    ent_coef: float = 0.01\n",
    "    \"\"\"coefficient of the entropy\"\"\"\n",
    "    vf_coef: float = 0.5\n",
    "    \"\"\"coefficient of the value function\"\"\"\n",
    "    max_grad_norm: float = 0.5\n",
    "    \"\"\"the maximum norm for the gradient clipping\"\"\"\n",
    "    target_kl: float = None\n",
    "    \"\"\"the target KL divergence threshold\"\"\"\n",
    "\n",
    "    # to be filled in runtime\n",
    "    batch_size: int = 0\n",
    "    \"\"\"the batch size (computed in runtime)\"\"\"\n",
    "    minibatch_size: int = 0\n",
    "    \"\"\"the mini-batch size (computed in runtime)\"\"\"\n",
    "    num_iterations: int = 0\n",
    "    \"\"\"the number of iterations (computed in runtime)\"\"\"\n",
    "\n",
    "\n",
    "def make_env(env_id, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\", continuous = False)\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\", continuous = False)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "args = Args()\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "args.num_iterations = args.total_timesteps // args.batch_size\n",
    "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "if args.track:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(\n",
    "        project=args.wandb_project_name,\n",
    "        entity=args.wandb_entity,\n",
    "        sync_tensorboard=True,\n",
    "        config=vars(args),\n",
    "        name=run_name,\n",
    "        monitor_gym=True,\n",
    "        save_code=True,\n",
    "    )\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-16T08:10:02.732885300Z",
     "start_time": "2025-01-16T08:10:02.721852300Z"
    }
   },
   "id": "461d9298ba48968e"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          x         y                                            frame 4  \\\n",
      "0  253.3333  256.4445  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "1  253.3333  256.0000  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "2  252.8889  256.0000  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "3  252.8889  256.0000  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "4  252.8889  256.0000  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "\n",
      "                                             frame 3  \\\n",
      "0  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "1  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "2  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "3  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "4  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "\n",
      "                                             frame 2  \\\n",
      "0  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "1  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "2  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "3  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "4  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "\n",
      "                                             frame 1  \n",
      "0  C:/Users/Yile0/PycharmProjects/osutime/frames/...  \n",
      "1  C:/Users/Yile0/PycharmProjects/osutime/frames/...  \n",
      "2  C:/Users/Yile0/PycharmProjects/osutime/frames/...  \n",
      "3  C:/Users/Yile0/PycharmProjects/osutime/frames/...  \n",
      "4  C:/Users/Yile0/PycharmProjects/osutime/frames/...  \n",
      "Index(['x', 'y', 'frame 4', 'frame 3', 'frame 2', 'frame 1'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8930it [02:04, 71.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# compile dataset.\n",
    "dataset_path = \"C:/Users/Yile0/PycharmProjects/osutime/map1_data.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)\n",
    "# small data for changing, basically just for trialing new changes.\n",
    "small_data = sklearn.utils.resample(data, n_samples= 1000)\n",
    "\n",
    "# frame 4 is the latest/ most recent.\n",
    "\n",
    "#originally these were one piece, changed for the dataloader to function\n",
    "processed_data = []\n",
    "processed_labels = []\n",
    "\n",
    "\n",
    "def process_img(paths):\n",
    "    # I had another self-made thing here that I decided to replace with premade functions\n",
    "    images = []\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    for path in paths:\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        # 100 by 75 because slightly better quality\n",
    "        # over 80 by 60\n",
    "        img = cv2.resize(img, (120, 68), interpolation=cv2.INTER_AREA)\n",
    "        img_normalized = cv2.normalize(img, None, 0, 1.0,\n",
    "                                       cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        images.append(transform(img_normalized))\n",
    "    out = torch.stack(images)\n",
    "    # i played with trying to reshape to 120 16, but I came back\n",
    "    # to this resolution because it was just so much better and faster\n",
    "    # with regards to my training speed.\n",
    "    out = out.reshape(len(paths),68,120)\n",
    "    # played with preloading here and loading later, seems like loading later is better.\n",
    "    # img = torch.from_numpy(img)\n",
    "    return out\n",
    "\n",
    "\n",
    "for index, row in tqdm(data.iterrows()):\n",
    "    # trying without normalization.\n",
    "    processed_labels.append([row['x'], row['y']])\n",
    "    # try to predict just on 1 frame for testing\n",
    "    processed_data.append(process_img([row['frame 4'],row['frame 3'],row['frame 2'],row['frame 1']]))\n",
    "    \n",
    "df = pd.DataFrame({'data':processed_data,\n",
    "                   'labels': processed_labels})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-16T07:41:31.801098100Z",
     "start_time": "2025-01-16T07:39:19.652663Z"
    }
   },
   "id": "5d70b6d3bff9c6a9"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "# Seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "# env setup\n",
    "env = OsuEnv(df)\n",
    "\n",
    "agent = Agent(env).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "\n",
    "# ALGO Logic: Storage setup\n",
    "obs = torch.zeros((args.num_steps, args.num_envs) + env.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((args.num_steps, 64,2)).to(device)\n",
    "logprobs = torch.zeros((args.num_steps, 64)).to(device)\n",
    "rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "dones = torch.zeros((args.num_steps, 4)).to(device)\n",
    "values = torch.zeros((args.num_steps, 64)).to(device)\n",
    "\n",
    "# TRY NOT TO MODIFY: start the game\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = env.reset()\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(args.num_envs).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-16T08:14:17.283132400Z",
     "start_time": "2025-01-16T08:14:17.133291600Z"
    }
   },
   "id": "26ae1c6ff9125cf1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bf0a85aa69bb17"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 4])\n",
      "torch.Size([4])\n",
      "torch.Size([128, 4])\n",
      "torch.Size([0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yile0\\AppData\\Local\\Temp\\ipykernel_31712\\2008877981.py:27: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (4) must match the existing size (0) at non-singleton dimension 0.  Target sizes: [4].  Tensor sizes: [0]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[74], line 14\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(dones\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(next_done\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m---> 14\u001B[0m dones[step] \u001B[38;5;241m=\u001B[39m next_done\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# ALGO LOGIC: action logic\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The expanded size of the tensor (4) must match the existing size (0) at non-singleton dimension 0.  Target sizes: [4].  Tensor sizes: [0]"
     ]
    }
   ],
   "source": [
    "for iteration in range(1, args.num_iterations + 1):\n",
    "        # Annealing the rate if instructed to do so.\n",
    "        if args.anneal_lr:\n",
    "            frac = 1.0 - (iteration - 1.0) / args.num_iterations\n",
    "            lrnow = frac * args.learning_rate\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        for step in range(0, args.num_steps):\n",
    "            global_step += args.num_envs\n",
    "            obs[step] = next_obs\n",
    "            \n",
    "            print(dones.shape)\n",
    "            print(next_done.shape)\n",
    "            dones[step] = next_done\n",
    "\n",
    "            # ALGO LOGIC: action logic\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            # TRY NOT TO MODIFY: execute the game and log data.\n",
    "            next_obs, reward, terminations, truncations, infos = env.step(action.cpu().numpy())\n",
    "            next_done = np.logical_or(terminations, truncations)\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n",
    "\n",
    "            if \"final_info\" in infos:\n",
    "                for info in infos[\"final_info\"]:\n",
    "                    if info and \"episode\" in info:\n",
    "                        print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "\n",
    "        # bootstrap value if not done\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "            for t in reversed(range(args.num_steps)):\n",
    "                if t == args.num_steps - 1:\n",
    "                    nextnonterminal = 1.0 - next_done\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "            returns = advantages + values\n",
    "\n",
    "        # flatten the batch\n",
    "        b_obs = obs.reshape((-1,) + env.single_observation_space.shape)\n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,) + env.single_action_space.shape)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        # Optimizing the policy and value network\n",
    "        b_inds = np.arange(args.batch_size)\n",
    "        clipfracs = []\n",
    "        for epoch in range(args.update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, args.batch_size, args.minibatch_size):\n",
    "                end = start + args.minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\n",
    "                logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if args.norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if args.clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -args.clip_coef,\n",
    "                        args.clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "            if args.target_kl is not None and approx_kl > args.target_kl:\n",
    "                break\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y  \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-16T08:14:34.054159Z",
     "start_time": "2025-01-16T08:14:34.017094700Z"
    }
   },
   "id": "8f893db9fc757834"
  },
  {
   "cell_type": "markdown",
   "source": [
    "closing things out. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78644498d78ea438"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " if args.save_model:\n",
    "    model_path = f\"runs/{run_name}/{args.exp_name}.cleanrl_model\"\n",
    "    torch.save(agent.state_dict(), model_path)\n",
    "    print(f\"model saved to {model_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-16T07:56:24.754012500Z",
     "start_time": "2025-01-16T07:56:24.754012500Z"
    }
   },
   "id": "b5251cb01a58579d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-16T07:52:49.862292200Z"
    }
   },
   "id": "1d0b47cc89dfb732"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-16T07:41:32.402227300Z"
    }
   },
   "id": "d1ff9b13c6bc84e5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
