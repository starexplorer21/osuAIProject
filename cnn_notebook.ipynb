{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-14T08:28:11.560462300Z",
     "start_time": "2025-01-14T08:28:06.236645800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "# Yile Shen\n",
    "# 3/7/2024\n",
    "# Advanced Programming: AIML\n",
    "# Osu playing robot\n",
    "\n",
    "# Load in relevant libraries, and alias where appropriate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import time\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# couple notes: i chose to switch to pytorch as I was more comfortable with it\n",
    "# and I needed to customize my model in a lot of ways, and I couldn't\n",
    "# adequately get tensorflow to work.\n",
    "\n",
    "# I also already had a really good setup for pytorch and I spent more time\n",
    "# tuning the gpu usage on tensorflow. Pytorch also saved WAY more memory overall.\n",
    "\n",
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# print name\n",
    "print(torch.cuda.get_device_name(device))\n",
    "\n",
    "# i had a tensorflow version, but I couldn't get some things to work in it\n",
    "# so I switched to pytorch because I'm more comfortable with it and can do more.\n",
    "\n",
    "#process images here so I can change resolution whenever\n",
    "#\n",
    "# FOLDER_PATH = \"C:/Users/Yile0/PycharmProjects/osutime/frames/\"\n",
    "#\n",
    "# for i in range(14, 5379):\n",
    "#     img = cv2.imread(FOLDER_PATH + str(i)+\".png\")\n",
    "#     img = cv2.resize(img, (160, 120), interpolation=cv2.INTER_AREA)\n",
    "#     cv2.imwrite(FOLDER_PATH + str(i)+\".png\", img)\n",
    "# making a custom image dataset class for pytorch\n",
    "# originally had a custom dataset maker, but the dataloader is way better\n",
    "# and formats them better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          x         y                                            frame 4  \\\n",
      "0  253.3333  256.4445  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "1  253.3333  256.0000  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "2  252.8889  256.0000  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "3  252.8889  256.0000  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "4  252.8889  256.0000  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "\n",
      "                                             frame 3  \\\n",
      "0  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "1  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "2  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "3  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "4  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "\n",
      "                                             frame 2  \\\n",
      "0  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "1  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "2  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "3  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "4  C:/Users/Yile0/PycharmProjects/osutime/frames/...   \n",
      "\n",
      "                                             frame 1  \n",
      "0  C:/Users/Yile0/PycharmProjects/osutime/frames/...  \n",
      "1  C:/Users/Yile0/PycharmProjects/osutime/frames/...  \n",
      "2  C:/Users/Yile0/PycharmProjects/osutime/frames/...  \n",
      "3  C:/Users/Yile0/PycharmProjects/osutime/frames/...  \n",
      "4  C:/Users/Yile0/PycharmProjects/osutime/frames/...  \n",
      "Index(['x', 'y', 'frame 4', 'frame 3', 'frame 2', 'frame 1'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8930it [01:57, 75.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# compile dataset.\n",
    "dataset_path = \"C:/Users/Yile0/PycharmProjects/osutime/map1_data.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "print(data.head())\n",
    "print(data.columns)\n",
    "# small data for changing, basically just for trialing new changes.\n",
    "small_data = sklearn.utils.resample(data, n_samples= 1000)\n",
    "\n",
    "# frame 4 is the latest/ most recent.\n",
    "\n",
    "#originally these were one piece, changed for the dataloader to function\n",
    "processed_data = []\n",
    "processed_labels = []\n",
    "\n",
    "\n",
    "def process_img(paths):\n",
    "    # I had another self-made thing here that I decided to replace with premade functions\n",
    "    images = []\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    for path in paths:\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        # 100 by 75 because slightly better quality\n",
    "        # over 80 by 60\n",
    "        img = cv2.resize(img, (120, 68), interpolation=cv2.INTER_AREA)\n",
    "        img_normalized = cv2.normalize(img, None, 0, 1.0,\n",
    "                                       cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        images.append(transform(img_normalized))\n",
    "    out = torch.stack(images)\n",
    "    # i played with trying to reshape to 120 16, but I came back\n",
    "    # to this resolution because it was just so much better and faster\n",
    "    # with regards to my training speed.\n",
    "    out = out.reshape(len(paths),68,120)\n",
    "    # played with preloading here and loading later, seems like loading later is better.\n",
    "    # img = torch.from_numpy(img)\n",
    "    return out\n",
    "\n",
    "\n",
    "for index, row in tqdm(data.iterrows()):\n",
    "    # trying without normalization.\n",
    "    processed_labels.append([row['x'], row['y']])\n",
    "    # try to predict just on 1 frame for testing\n",
    "    processed_data.append(process_img([row['frame 4'],row['frame 3'],row['frame 2'],row['frame 1']]))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_data, processed_labels, test_size=0.2, random_state=39)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T08:30:09.358056100Z",
     "start_time": "2025-01-14T08:28:11.562461800Z"
    }
   },
   "id": "e4982317ce4ad637"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, imageTransform=None, num_workers=0):\n",
    "        self.imageTransform = imageTransform\n",
    "        self.num_workers = num_workers\n",
    "        self.imgs = images\n",
    "        self.targets = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.imgs[idx]\n",
    "        target = self.targets[idx]\n",
    "        label = torch.Tensor(target)\n",
    "\n",
    "        return image, label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T08:30:09.375055500Z",
     "start_time": "2025-01-14T08:30:09.358056100Z"
    }
   },
   "id": "3238d9ace87644e3"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Creating a CNN class\n",
    "# conv neural net combined was bad, this iteration separates them\n",
    "class ConvNeuralNet(nn.Module):\n",
    "    #  Determine what layers and their order in CNN object\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "        # adjusted the first pooling to be 4 instead.\n",
    "        # tried second pooling to be 4 too\n",
    "\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=4, out_channels=64, kernel_size=4)\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=4)\n",
    "        self.max_pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=512, out_channels=64, kernel_size=4)\n",
    "        self.max_pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.flatten = nn.Flatten(1)\n",
    "        self.fc1 = nn.Linear(24192, 128)\n",
    "        # originally tried relu layers, but wanted something non-linear\n",
    "        # went back because elu wasn't doing better\n",
    "        # had 3 layers to begin with, then tried 4, then tried 7\n",
    "        # I one off tried 13 but it didn't fit.=\n",
    "        self.fc2 = nn.Linear(128,2)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, 16)\n",
    "        self.fc7 = nn.Linear(16, 2)\n",
    "        # self.relu7 = nn.ReLU()\n",
    "        # self.fc8 = nn.Linear(75, 1)\n",
    "        # self.relu8 = nn.ReLU()\n",
    "        # self.fc9 = nn.Linear(25, 1)\n",
    "        # self.relu9 = nn.ReLU()\n",
    "        # self.fc10 = nn.Linear(50, 1)\n",
    "        self.mult = torch.tensor([512,384])\n",
    "        self.mult = self.mult.to(device)\n",
    "    # Progresses data across layers\n",
    "    def forward(self, input):\n",
    "        out_x = self.conv_layer1(input)\n",
    "        out_x = self.relu(out_x)\n",
    "        out_x = self.max_pool1(out_x)\n",
    "        out_x = self.conv_layer2(out_x)\n",
    "        out_x = self.relu(out_x)\n",
    "        out_x = self.max_pool2(out_x)\n",
    "\n",
    "        # out_x = self.conv_layer3(out_x)\n",
    "        # out_x = self.relu(out_x)\n",
    "        # out_x = self.max_pool3(out_x)\n",
    "        # out_x = self.conv_layer4(out_x)\n",
    "        # out_x = self.relu(out_x)\n",
    "        # out_x = self.max_pool4(out_x)\n",
    "\n",
    "        out_x = self.flatten(out_x)\n",
    "        \n",
    "        out_x = self.fc1(out_x)\n",
    "        out_x = self.sigmoid(out_x)\n",
    "        out_x = self.fc2(out_x)\n",
    "        out_x = self.sigmoid(out_x)\n",
    "        out_x = out_x * self.mult\n",
    "        # out_x = self.fc3(out_x)\n",
    "        # out_x = self.sigmoid(out_x)\n",
    "        # out_x = self.relu(out_x)\n",
    "        # out_x = self.relu(out_x)\n",
    "        # out_x = self.relu(out_x)\n",
    "\n",
    "        return out_x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T08:31:27.834580500Z",
     "start_time": "2025-01-14T08:31:27.824581200Z"
    }
   },
   "id": "687719ad30eaa4b4"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Define relevant variables for the ML task\n",
    "# tried batch sizes of 32 and 128 as well, but this was best\n",
    "# 128 had better cuda utilization but didn't boost speed too much.\n",
    "# decided it was more worth it to have better accuracy because 128 could\n",
    "# introduce inaccuracies.\n",
    "# after adding a few new layers 64 wasn't enough to have fast epochs\n",
    "# so I decided to move to 128.\n",
    "# accidentally tried batch size of 1, was way too inefficient\n",
    "batch_size = 80\n",
    "# originally learning rate was 0.001, but im making it learn longer and slower.\n",
    "# makes me wonder if my original idea would have worked, but now it's too late to fix it.\n",
    "# 0.001 basically didn't learn. I trained for somwhere around 500 epochs and loss basiclly didn't change\n",
    "# from now on, I ran some 20 epoch experiments.\n",
    "# 0.0001 was learning a lot to start, but loss seems to cycle between 0.10 and 0.13 or so\n",
    "\n",
    "# 0.00001 had loss settle at around 0.28/0.3, which is maybe due to the lack of time it had to train.\n",
    "# loss was generally going down even at 50 epochs, can't tell without a longer experiment.\n",
    "# i'm going to keep it at this and train for 100 and see what happens.\n",
    "# doesn't seem to improve past loss = 0.11\n",
    "\n",
    "# running a 400 epoch experiment with learning rate 0.000001\n",
    "learning_rate = 0.0000001\n",
    "# ran a few experiments on 20 epochs to see if it would learn.\n",
    "num_epochs = 1500"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T08:31:28.362280900Z",
     "start_time": "2025-01-14T08:31:28.354281600Z"
    }
   },
   "id": "d6a7c45f22aa9850"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yile0\\AppData\\Local\\Temp\\ipykernel_46492\\1901619840.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('model_duo.pth')\n"
     ]
    }
   ],
   "source": [
    "dataset = ImageDataset(X_train, y_train)\n",
    "test = ImageDataset(X_test, y_test)\n",
    "\n",
    "# added workers to speedup epoch time.\n",
    "# 3 is the max tolerable it seems.\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# model = ConvNeuralNet(2, train_data.shape[0])\n",
    "\n",
    "# Set Loss function with criterion\n",
    "\n",
    "model = ConvNeuralNet(2)\n",
    "model.cuda(device)\n",
    "# adam is just better, tried other optimizers like sgd though\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# MSE loss instead of L1, which is squared loss vs linear loss.\n",
    "# Huber over both, has the benefits of both.\n",
    "criterion = nn.MSELoss()\n",
    "epoch_start = 0\n",
    "\n",
    "\n",
    "epoch_start = 0\n",
    "# added code that allows it to train again from a checkpoint\n",
    "# # it lets me train in chunks over time.\n",
    "checkpoint = torch.load('model_duo.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch_start = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "# Set optimizer with optimizer\n",
    "\n",
    "total_step = len(processed_data)\n",
    "\n",
    "model.train()\n",
    "\n",
    "losses = []\n",
    "losses_val = []\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T08:31:29.105930600Z",
     "start_time": "2025-01-14T08:31:28.932104300Z"
    }
   },
   "id": "e63d58e767afaafd"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([24, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([80, 24192])\n",
      "torch.Size([26, 24192])\n",
      "val\n",
      "tensor([[119.1111,  85.7778],\n",
      "        [467.5555, 204.0000],\n",
      "        [174.2222, 118.6667],\n",
      "        [292.0000,  76.0000],\n",
      "        [204.0000, 274.2222],\n",
      "        [233.7778, 103.1111],\n",
      "        [230.2222,  59.5556],\n",
      "        [406.6667, 298.2222],\n",
      "        [431.1111, 155.5556],\n",
      "        [ 79.1111, 170.2222],\n",
      "        [418.6667, 268.0000],\n",
      "        [423.5555,  90.6667],\n",
      "        [285.3333,  59.1111],\n",
      "        [309.3333, 188.4444],\n",
      "        [108.8889, 308.0000],\n",
      "        [252.8889, 171.1111],\n",
      "        [422.6667, 244.0000],\n",
      "        [207.1111,  37.3333],\n",
      "        [252.8889, 124.8889],\n",
      "        [372.8889,  48.8889],\n",
      "        [443.5555, 328.4445],\n",
      "        [101.3333, 200.8889],\n",
      "        [357.3333,  36.8889],\n",
      "        [463.5555,  48.4444],\n",
      "        [ 94.6667, 300.4445],\n",
      "        [335.5555,  96.0000]], device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1500 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 73\u001B[0m\n\u001B[0;32m     64\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave({\n\u001B[0;32m     65\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m'\u001B[39m: epoch,\n\u001B[0;32m     66\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_state_dict\u001B[39m\u001B[38;5;124m'\u001B[39m: model\u001B[38;5;241m.\u001B[39mstate_dict(),\n\u001B[0;32m     67\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124moptimizer_state_dict\u001B[39m\u001B[38;5;124m'\u001B[39m: optimizer\u001B[38;5;241m.\u001B[39mstate_dict(),\n\u001B[0;32m     68\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m: criterion,\n\u001B[0;32m     69\u001B[0m             }, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_duo.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m---> 73\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[12], line 45\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28mprint\u001B[39m(labels)\n\u001B[1;32m---> 45\u001B[0m \u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;66;03m# step lr scheduler\u001B[39;00m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# add so I can graph later\u001B[39;00m\n\u001B[0;32m     49\u001B[0m losses\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\_tensor.py:523\u001B[0m, in \u001B[0;36mTensor.__repr__\u001B[1;34m(self, tensor_contents)\u001B[0m\n\u001B[0;32m    519\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    520\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__repr__\u001B[39m, (\u001B[38;5;28mself\u001B[39m,), \u001B[38;5;28mself\u001B[39m, tensor_contents\u001B[38;5;241m=\u001B[39mtensor_contents\n\u001B[0;32m    521\u001B[0m     )\n\u001B[0;32m    522\u001B[0m \u001B[38;5;66;03m# All strings are unicode in Python 3.\u001B[39;00m\n\u001B[1;32m--> 523\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tensor_str\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_str\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensor_contents\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_contents\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\_tensor_str.py:708\u001B[0m, in \u001B[0;36m_str\u001B[1;34m(self, tensor_contents)\u001B[0m\n\u001B[0;32m    706\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad(), torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39m_python_dispatch\u001B[38;5;241m.\u001B[39m_disable_current_modes():\n\u001B[0;32m    707\u001B[0m     guard \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_DisableFuncTorch()\n\u001B[1;32m--> 708\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_str_intern\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensor_contents\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_contents\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\_tensor_str.py:625\u001B[0m, in \u001B[0;36m_str_intern\u001B[1;34m(inp, tensor_contents)\u001B[0m\n\u001B[0;32m    623\u001B[0m                     tensor_str \u001B[38;5;241m=\u001B[39m _tensor_str(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_dense(), indent)\n\u001B[0;32m    624\u001B[0m                 \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 625\u001B[0m                     tensor_str \u001B[38;5;241m=\u001B[39m \u001B[43m_tensor_str\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayout \u001B[38;5;241m!=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstrided:\n\u001B[0;32m    628\u001B[0m     suffixes\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayout=\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayout))\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\_tensor_str.py:357\u001B[0m, in \u001B[0;36m_tensor_str\u001B[1;34m(self, indent)\u001B[0m\n\u001B[0;32m    353\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _tensor_str_with_formatter(\n\u001B[0;32m    354\u001B[0m         \u001B[38;5;28mself\u001B[39m, indent, summarize, real_formatter, imag_formatter\n\u001B[0;32m    355\u001B[0m     )\n\u001B[0;32m    356\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 357\u001B[0m     formatter \u001B[38;5;241m=\u001B[39m \u001B[43m_Formatter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mget_summarized_data\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msummarize\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    358\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _tensor_str_with_formatter(\u001B[38;5;28mself\u001B[39m, indent, summarize, formatter)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\_tensor_str.py:191\u001B[0m, in \u001B[0;36m_Formatter.__init__\u001B[1;34m(self, tensor)\u001B[0m\n\u001B[0;32m    189\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    190\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m value \u001B[38;5;129;01min\u001B[39;00m nonzero_finite_vals:\n\u001B[1;32m--> 191\u001B[0m                 value_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;130;43;01m{{\u001B[39;49;00m\u001B[38;5;124;43m:.\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mPRINT_OPTS\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprecision\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;130;43;01m}}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    192\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_width \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_width, \u001B[38;5;28mlen\u001B[39m(value_str))\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m PRINT_OPTS\u001B[38;5;241m.\u001B[39msci_mode \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\_tensor.py:1052\u001B[0m, in \u001B[0;36mTensor.__format__\u001B[1;34m(self, format_spec)\u001B[0m\n\u001B[0;32m   1050\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(Tensor\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__format__\u001B[39m, (\u001B[38;5;28mself\u001B[39m,), \u001B[38;5;28mself\u001B[39m, format_spec)\n\u001B[0;32m   1051\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_meta \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m Tensor:\n\u001B[1;32m-> 1052\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__format__\u001B[39m(format_spec)\n\u001B[0;32m   1053\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mobject\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__format__\u001B[39m(\u001B[38;5;28mself\u001B[39m, format_spec)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "def main():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    for epoch in tqdm(range(epoch_start, epoch_start + num_epochs)):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        # Load in the data in batches using the train_loader object\n",
    "        for i, (images, labels) in enumerate(data_loader):\n",
    "            images = images.to(device)\n",
    "            # Move tensors to the configured device\n",
    "            # images already got loaded on\n",
    "            # images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass\n",
    "            output = model(images)\n",
    "            # process into the separate multipliers. \n",
    "            \n",
    "            loss = criterion(output, labels)\n",
    "            # had tried to combined, but now i'm just completely\n",
    "            # separating the two models.\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # added checkpoint saver\n",
    "        if (epoch + 1) % 6 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,\n",
    "            }, \"model_duo.pth\")\n",
    "        model.eval()\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Move tensors to the configured device\n",
    "            # images already got loaded on\n",
    "            # images = images.to(device)\n",
    "            output = model(images)\n",
    "            # process into the separate multipliers. \n",
    "            loss_valid = criterion(output, labels)\n",
    "            \n",
    "        print('val')\n",
    "        print(labels)\n",
    "        print(output)\n",
    "\n",
    "        # step lr scheduler\n",
    "        # add so I can graph later\n",
    "        losses.append(loss.item())\n",
    "        losses_val.append(loss_valid.item())\n",
    "        print('validate Loss_X: {:.10f}'.format(loss_valid.item()))\n",
    "\n",
    "        end = time.time()\n",
    "        print('Epoch [{}/{}], Loss1: {:.10f}, Time: {:.10f}'.format(epoch + 1, num_epochs + epoch_start, loss.item(), end-start))\n",
    "\n",
    "    # torch.save({\n",
    "    #                 'epoch': num_epochs,\n",
    "    #                 'model_x_state_dict': model_x.state_dict(),\n",
    "    #                 'optimizer_x_state_dict': optimizer_x.state_dict(),\n",
    "    #                 'model_y_state_dict': model_y.state_dict(),\n",
    "    #                 'optimizer_y_state_dict': optimizer_y.state_dict(),\n",
    "    #                 'loss': criterion,\n",
    "    #             }, \"model.pth\")\n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,\n",
    "            }, \"model_duo.pth\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T08:31:32.907389900Z",
     "start_time": "2025-01-14T08:31:29.829358400Z"
    }
   },
   "id": "23e84f7858f74ce0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.plot(losses_val, '-.')\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title('losses')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for i, (labels, images) in enumerate(processed_data):\n",
    "#         images = images.reshape((1, 4, 60, 80))\n",
    "#         images = images.to(device)\n",
    "#         labels = torch.tensor(labels, dtype=torch.float32)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(images)\n",
    "#         print([labels[0], labels[1]], [outputs[0], outputs[1]])\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1e3c3146b1737bfb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4aae66415c44aba2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7185e098bb39f498"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
